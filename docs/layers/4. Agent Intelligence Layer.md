# 4. Agent Intelligence Layer - Design Document

**Version:** 2 (Revised for Alpha Roadmap Alignment)
**Last Updated:** 2/10/2025

---

## 1. Overview & Purpose

*   The Agent Intelligence Layer (AIL), implemented primarily in **Python**, is responsible for the decision-making processes of the agents within the Fish Eco Sim. It receives observations about an agent's state and its environment (from the Simulation Layer via the Orchestration Layer), processes this information, and selects an action for the agent to perform.
*   Its primary role is to host the "brains" of the agents, which will be implemented as Deep Reinforcement Learning (DRL) neural networks using frameworks like PyTorch or TensorFlow. This layer will also manage the training loop for these DRL agents, including experience collection, neural network model updates, and policy improvement.
*   Key goals include: implementing effective DRL algorithms, enabling agents to learn complex behaviors, managing neural network models (training, saving, loading), and interacting efficiently with the Orchestration Layer to receive observations/rewards and send actions.

---

## 2. Core Components & Functionality (Targeted for Alpha PoC `a0.8.Y.Z`)

*   **Agent Brain Interface/Wrapper (Target: Part of `a0.8.1.Z` & `a0.8.3.Z`):**
    *   A common Python class structure representing an "Agent Brain."
    *   Handles receiving an observation (as a tensor/array) and returning a selected action.
    *   Encapsulates the neural network model and any associated logic for action selection (e.g., exploration strategies).
    *   Manages recurrent state for the neural network if Long Short-Term Memory (LSTM) / Gated Recurrent Unit (GRU) layers are used.
*   **Neural Network Model(s) (Target: `a0.8.1.Z`):**
    *   Defined using PyTorch or TensorFlow/Keras.
    *   Initial Alpha architecture will be simple (e.g., a few fully connected layers) designed based on the Alpha `AgentObservationData` and `AgentActionInput` (from `a0.4.7.Z`).
    *   (Future Beta/GA) Separate or more complex models might exist for different agent types or if evolving architectures.
*   **Inference Engine (Target: `a0.8.3.Z`):**
    *   Logic within the Agent Brain wrapper to perform a forward pass of its neural network given an observation to produce action logits or Q-values.
    *   Includes action selection logic (e.g., sampling from a probability distribution for policy gradients, epsilon-greedy for Q-learning).
    *   (Future Beta) May handle batching of observations for efficient GPU utilization if multiple agents' decisions are processed by a single AI service instance.
*   **Reinforcement Learning Algorithm Implementation (Target: `a0.8.3.Z`, `a0.8.4.Z`):**
    *   Implementation of a chosen simple DRL algorithm for Alpha (e.g., REINFORCE, basic Deep Q-Network (DQN)). This might leverage components from libraries like Stable Baselines3 or be a focused custom implementation.
    *   **Policy Function/Q-Function:** The neural network model itself.
    *   **Value Function (if applicable, e.g., Actor-Critic):** May be part of the neural network or a separate network.
    *   **Loss Calculation:** Functions to compute the loss based on the DRL algorithm's objectives (e.g., policy gradient loss, Mean Squared Bellman Error).
    *   **Optimizer:** Uses PyTorch/TensorFlow optimizers (e.g., Adam, SGD) to update neural network weights.
*   **Experience Replay Buffer (for off-policy algorithms like DQN - Target: `a0.8.4.Z`):**
    *   Stores `(Observation, Action, Reward, Next Observation, Done)` tuples.
    *   Provides methods for adding new experiences and sampling random batches for training.
*   **Training Loop Manager (Target: `a0.8.4.Z`):**
    *   The main Python script or class that orchestrates the DRL training process for an agent or set of agents.
    *   Communicates with the Experiment Management System (`a0.6`) to initiate simulation episodes for experience gathering.
    *   Receives observation and reward data from the Orchestration Layer (via NATS).
    *   Selects actions (using inference) and sends them to the Orchestration Layer (via NATS).
    *   Collects and stores experiences.
    *   Periodically triggers training updates on the neural network model(s).
    *   Manages model saving.
*   **Model Management (Target: `a0.8.4.Z`):**
    *   Functions for saving trained neural network model weights (and optimizer states if needed for resuming training).
    *   Functions for loading pre-trained models for inference or continued training.
    *   (Future) More sophisticated versioning or tracking of models.
*   **Observation & Action Space Adapters (Target: `a0.8.2.Z`):**
    *   Code to convert incoming `AgentObservation` Protobuf messages into NumPy arrays or PyTorch/TensorFlow tensors suitable for the neural network input. Includes any necessary normalization.
    *   Code to convert the neural network's output (e.g., action logits, Q-values) into an `AgentAction` Protobuf message to be sent out.

---

## 3. Key Technologies & Patterns

*   **Primary Language(s):** Python
*   **Key Libraries/Frameworks (Anticipated for Alpha):**
    *   **PyTorch** (preferred for research flexibility) or **TensorFlow/Keras**.
    *   **NumPy** (for numerical operations, tensor manipulation).
    *   (Potentially) **Stable Baselines3 (SB3)** for a robust implementation of a simple DRL algorithm, or use as a reference.
    *   `nats-py` (for NATS communication with the Orchestration Layer).
    *   `protobuf` (Python library for Protobuf messages).
    *   `PyYAML` (for loading its own configuration).
*   **Architectural Patterns Used (within this layer):**
    *   Reinforcement Learning Agent architecture (Policy, Value Function, Replay Buffer as applicable).
    *   Deep Neural Network for function approximation.
*   **Data Formats Handled (Internal):** NumPy arrays, PyTorch/TensorFlow Tensors, Python lists/dictionaries for experiences.

---

## 4. Interactions & Interfaces (Focus on Alpha `a0.8.Y.Z` integration)

*   **Communication with Orchestration Layer / Communication Layer (via NATS, managed by Go Orchestration Layer):**
    *   **Events/Data Subscribed To (received as `GenericEventRequest` payloads from Orchestration Layer):**
        *   `event_type_key: "sim.agent_observation.v1"`
            *   Description: Observation vector for a specific agent from the Simulation Layer.
            *   Expected Payload Schema: `AgentObservation` Protobuf message.
        *   `event_type_key: "sim.agent_reward.v1"`
            *   Description: Reward signal and done flag for an agent after its last action.
            *   Expected Payload Schema: `AgentReward` Protobuf message.
        *   (Potentially) `command_key: "ai.control.set_mode.v1"` (e.g., from Experiment Management System to switch between training/inference).
        *   (Potentially) `command_key: "ai.control.load_model.v1"` (to load specific neural network weights).
    *   **Events Published (as `GenericEventRequest` payloads to Orchestration Layer):**
        *   `event_type_key: "ai.agent_action.v1"`
            *   Description: Action chosen by the agent's intelligence.
            *   Payload Schema: `AgentAction` Protobuf message.
        *   `event_type_key: "ai.training_progress.v1"` (Optional, for monitoring/UI display)
            *   Description: Periodic updates on training status (e.g., episode count, average reward, loss).
            *   Payload Schema: `TrainingProgressUpdate` Protobuf message.
*   **Interaction with Configuration Management System:**
    *   Reads its configuration from YAML files (potentially passed by Experiment Management System or loaded directly).
    *   Example Parameters: Neural Network architecture details (layers, units, activations), learning rate, discount factor (gamma), batch size, replay buffer size, exploration parameters (epsilon), choice of DRL algorithm.
*   **Interaction with Logging Layer (Indirect):**
    *   For DRL algorithms that can learn from offline batch data, this layer might eventually read processed experiences from the Parquet files generated by the Logging Layer.
    *   Its own `ai.training_progress.v1` events, once published to NATS via the Orchestration Layer, can be captured by the Logging Layer.
*   **Interaction with Experiment Management System (`a0.6.Y.Z`):**
    *   The training loop within the Agent Intelligence Layer will likely request the Experiment Management System to run simulation episodes for experience gathering.
*   **Interaction with Monitoring & Alerting System:**
    *   Expose metrics (e.g., average reward, loss, exploration rate) via a Prometheus endpoint or by publishing specific metric events to NATS.
    *   Potential alert conditions: Training loss exploding/stagnating, reward consistently very low during training.

---

## 5. Data Management & State

*   **Persistent State:**
    *   Saved Neural Network model weights (e.g., PyTorch `.pt` or `.pth` files, TensorFlow SavedModel format).
    *   Saved Optimizer states (to allow resuming training).
    *   (Potentially for off-policy) Saved Replay Buffer contents (can be very large).
    *   Logs of training metrics and progress.
*   **In-Memory State:**
    *   Current Neural Network model weights being trained/used for inference.
    *   Optimizer state.
    *   Replay Buffer contents (if off-policy).
    *   Recurrent states of Neural Networks (e.g., LSTM hidden states) per agent if applicable.
    *   Current batch of experiences being processed for a training update.
*   **Data Consistency:** Critical for replay buffers and ensuring model updates are based on correct data. Standard DRL algorithm implementations address this.

---

## 6. Scalability & Performance Considerations

*   **Training Performance:**
    *   DRL training is computationally intensive. GPU acceleration is essential for deep neural networks.
    *   **Bottlenecks:** Experience collection (simulation speed), data transfer between Simulation Layer and Agent Intelligence Layer (via Orchestration Layer), neural network forward/backward passes.
*   **Inference Performance:**
    *   Must be fast enough to provide actions to the Simulation Layer without becoming the primary simulation speed bottleneck. Batching observations for inference on GPU can help.
*   **Strategies for Scaling/Performance (Many are Beta/Future concerns):**
    *   **Distributed DRL (e.g., Ray RLlib):** For scaling training.
    *   Efficient data transfer (already addressed with Protobuf/FlatBuffers, NATS).
    *   Asynchronous processing of observations/actions to decouple AI decision time from simulation tick strictness.
*   **Memory Usage:** Neural network models and especially replay buffers can be memory-intensive.

---

## 7. Testing Strategy (Aligned with Alpha Roadmap `a0.8.Y.Z`)

*   **Unit Tests (`a0.8.1t` - `a0.8.4t`):**
    *   Neural Network layer implementations, forward pass.
    *   Action selection logic (including exploration).
    *   Loss function calculations.
    *   Replay buffer add/sample logic.
    *   Optimizer step application.
    *   Model saving/loading.
    *   NATS message serialization/deserialization for observations/actions.
*   **Integration Tests (Intra-layer):**
    *   Test a chosen DRL algorithm on a standard, simple environment (e.g., OpenAI Gym's CartPole, or a custom minimal Python environment) to verify it can learn.
*   **Integration Tests (Inter-layer via Mocks - `a0.8.2t` and part of `a0.8.5t`):**
    *   Connect Agent Intelligence Layer to a mock Orchestration Layer. Send mock observations/rewards via NATS, verify correct `AgentAction` Protobuf messages are published.
*   **Model Validation / End-to-End Learning Test (`a0.8.5.Z`):**
    *   Run the full integrated system (Sim -> Orch -> AI -> Orch -> Sim) on a simple task.
    *   Monitor if learning occurs (e.g., rewards trend upwards).

---

## 8. Future Enhancements / Open Questions

*   **Multi-Agent Reinforcement Learning (MARL).**
*   **Hierarchical Reinforcement Learning (HRL).**
*   **Curriculum Learning.**
*   **Evolutionary Algorithms for optimizing Neural Network (Neuroevolution) - synergy with Evolution Engine.**
*   Support for more DRL algorithms and NN architectures.
*   Integration with experiment tracking tools (MLflow, Weights & Biases) for logging DRL metrics, hyperparameters, and models.

---

## Alpha Roadmap Version Breakdown for Agent Intelligence Layer Development:

This section explicitly lists the tasks from the main Alpha Roadmap (`alpha.md`) that pertain to the development and integration of the Agent Intelligence Layer.

*   **`a0.2.Y.Z`: Communication Layer - Core Message Schemas & Interface Definitions**
    *   `a0.2.1.Z`: Define Core Event & Data Transfer Payload Schemas (defines `AgentObservation`, `AgentAction`, `AgentReward` Protobufs used by this layer).
    *   `a0.2.2.Z`: Define Initial NATS Topic Structures (defines NATS topics this layer will use for obs/act/rew).
*   **`a0.8.Y.Z`: Agent Intelligence Layer (Python) - Basic DRL Neural Net & Training Loop**
    *   `a0.8.1.Z`: DRL Framework Setup & Basic Neural Network Definition
        *   (`a0.8.1.1t` & `a0.8.1.1` through `a0.8.1.3t` & `a0.8.1.3` as detailed previously)
    *   `a0.8.2.Z`: Communication Integration (Receiving Obs/Rewards, Sending Actions via NATS)
        *   (`a0.8.2.1t` & `a0.8.2.1` through `a0.8.2.2t` & `a0.8.2.2` as detailed previously)
    *   `a0.8.3.Z`: Implement Basic DRL Algorithm & Inference Path
        *   (`a0.8.3.1t` & `a0.8.3.1` through `a0.8.3.2t` & `a0.8.3.2` as detailed previously)
    *   `a0.8.4.Z`: Implement Training Loop & Model Persistence
        *   (`a0.8.4.1t` & `a0.8.4.1` through `a0.8.4.4t` & `a0.8.4.4` as detailed previously)
    *   `a0.8.5.Z`: Initial End-to-End Learning Test
        *   (`a0.8.5.1t` & `a0.8.5.1` through `a0.8.5.2t` & `a0.8.5.2` as detailed previously)

---