# 2. Orchestration Layer - Design Document

**Version:** 2 (Revised for Alpha Roadmap Alignment)
**Last Updated:** 2/10/2025

---

## 1. Overview & Purpose

*   The Orchestration Layer, implemented in **Go**, acts as the central coordinator and manager for the entire Fish Eco Sim distributed system. It is responsible for the lifecycle of Simulation Worker processes (Rust executables), managing communication flows via NATS and gRPC, and exposing control interfaces to clients like the UI Layer and Experiment Management System.
*   Its primary role is to enable multiple simulation instances to run (potentially in parallel during Beta/General Availability phases) and to connect them with other components (UI Layer, Agent Intelligence Layer, Logging Layer) in a decoupled manner. It aims to be simulation-agnostic ("armless"), capable of managing different types of worker processes based on configuration.
*   Key goals include: robust (though initially simple for Alpha) worker management, efficient and reliable event/command routing using defined Protobuf/FlatBuffer schemas, providing a stable Application Programming Interface (API) for control and data subscription, and laying the groundwork for future scalability.

---

## 2. Core Components & Functionality (Targeted for Alpha PoC `a0.3.Y.Z` & integration with `a0.5.Y.Z`, `a0.6.Y.Z`)

*   **Configuration Loader (Target: `a0.3.1.Z`):**
    *   Loads its own operational configuration (e.g., NATS server address, gRPC listen ports for worker and client interfaces) from YAML files.
    *   Loads definitions for simulation worker types it can manage (e.g., path to Rust executable, default startup arguments).
*   **NATS Connection Manager (Target: `a0.3.1.Z`):**
    *   Establishes and maintains a connection to the NATS server (and NATS JetStream for logging persistence).
*   **gRPC Service Endpoints (Server Implementation - Worker Interface: `a0.3.2.Z`, Client Interface: `a0.3.5.Z`):**
    *   Uses gRPC service definitions from `a0.2.2.Z`.
    *   **`OrchestratorWorkerService` (for Simulation Workers):**
        *   Implements `RegisterWorker()`: Allows Rust Simulation Workers to announce their presence and `simulation_id`.
        *   Implements `ReportEvent()`: Endpoint for Simulation Workers to publish their `GenericEventRequest` messages.
    *   **`OrchestratorClientService` (for UI Layer, Experiment Management System, other tools):**
        *   Implements `StartSimulation(config)`: Receives a request to start a new simulation run, triggers Worker Manager.
        *   Implements `ControlSimulation(sim_id, control_command)`: Receives commands (pause, step, reset) for a specific simulation.
        *   (Alpha PoC might simplify or stub `SubscribeToEvents` and `GetSimulationStatus`, focusing on command initiation).
*   **Worker Manager (Minimal for Alpha - Target: `a0.3.3.Z`):**
    *   Responsible for spawning a single external Rust Simulation Worker process based on configuration.
    *   Passes necessary startup configuration to the worker (e.g., its assigned `worker_id`, `simulation_id`, Orchestration Layer's gRPC address, NATS command topic).
    *   Basic monitoring: Detects if the worker process exits and logs this. (Complex pooling/restarts deferred).
    *   Tracks the state of the single active worker (e.g., which `simulation_id` it's running).
*   **Event Ingestion & Pub/Sub Hub (NATS Integration - Target: `a0.3.4.Z`):**
    *   When `ReportEvent()` is called by a Simulation Worker, this component:
        *   Deserializes the outer `GenericEventRequest` (if needed to inspect `event_type_key` for routing).
        *   Publishes the `GenericEventRequest` (or its `event_payload`) to appropriate NATS topics based on `event_type_key` and `simulation_id` (using topic structure from `a0.2.2.1`).
        *   Ensures events destined for logging are published to the designated NATS JetStream topic.
*   **Command Router/Forwarder (Target: `a0.3.6.Z`):**
    *   When `ControlSimulation()` or other command RPCs are called by a client:
        *   Identifies the target Simulation Worker (for Alpha, only one active worker).
        *   Forwards the command to that worker (e.g., by publishing a command message to the worker's dedicated NATS command topic).

---

## 3. Key Technologies & Patterns

*   **Primary Language(s):** Go
*   **Key Libraries/Frameworks (Anticipated for Alpha):**
    *   Go standard library (`os/exec` for worker spawning, `net/http` for Prometheus metrics).
    *   `grpc-go` (for implementing gRPC services).
    *   `nats.go` (NATS client, potentially embedded NATS server for simple local development, NATS JetStream for logging persistence).
    *   `Goroutines` and `Channels` (for concurrent handling of gRPC requests, NATS messages, worker monitoring).
    *   `gopkg.in/yaml.v2` or `v3` (for parsing its own YAML configuration).
    *   `client_golang/prometheus` (for exposing `/metrics`).
*   **Architectural Patterns Used (within this layer):**
    *   Central Service / Hub.
    *   Publish-Subscribe Broker (leveraging NATS).
    *   Process Management (basic, for the worker).
    *   Request/Response (for gRPC services).
*   **Data Formats Handled (Internal):** Go structs representing worker/client connections, subscriptions. (External: Protobuf `GenericEventRequest` and other defined gRPC message types).

---

## 4. Interactions & Interfaces (Focus on Alpha `a0.3.Y.Z` implementation & consuming `a0.2.Y.Z` definitions)

*   **Communication with Simulation Workers (Rust - as defined in `a0.2.2.Z` and implemented by worker in `a0.5.Y.Z`):**
    *   **Implements Server for:** `OrchestratorWorkerService.RegisterWorker()`, `OrchestratorWorkerService.ReportEvent()`.
    *   **Publishes Commands To Worker (via NATS topic specified to worker at startup):**
        *   `event_type_key: "orchestrator.control.step"`
        *   `event_type_key: "orchestrator.control.pause"` / `"orchestrator.control.resume"`
        *   `event_type_key: "orchestrator.ai.action"` (relaying action from Agent Intelligence Layer)
*   **Communication with Client Layers (UI Layer, Experiment Management System - Python/C# - as defined in `a0.2.2.Z`):**
    *   **Implements Server for:** `OrchestratorClientService.StartSimulation()`, `OrchestratorClientService.ControlSimulation()`.
    *   **Publishes Events To Clients (via NATS topics defined in `a0.2.2.1`):**
        *   `events.simulation.{sim_id}.world_state` (payload: `WorldStateUpdate` FlatBuffer within `GenericEventRequest`)
        *   `events.simulation.{sim_id}.agent.{agent_id}.observation` (payload: `AgentObservation` Protobuf within `GenericEventRequest`)
        *   `events.simulation.{sim_id}.agent.{agent_id}.reward` (payload: `AgentReward` Protobuf within `GenericEventRequest`)
        *   Other simulation events like `agent_died`, `agent_spawned`.
*   **Communication with Logging Layer (Python - target `a0.7.Y.Z`):**
    *   **Publishes (via NATS JetStream topic like `logging.events.raw`):** All `GenericEventRequest` messages received from Simulation Workers that are marked for logging.
*   **Communication with Monitoring & Alerting System (target `a0.X_Monitoring`):**
    *   **Exposes:** Prometheus `/metrics` endpoint.
    *   **Publishes (via NATS):** Critical operational alert events (e.g., `orchestrator.worker_crash`).
*   **Interaction with Configuration Management System (target `a0.1.Y.Z` and `a0.11.Y.Z`):**
    *   Reads its own service configuration (NATS URL, gRPC ports, worker executable paths) from a YAML file.
    *   Receives full simulation configurations (as part of `StartSimulation` command from Experiment Management System) which it then passes to Simulation Workers.

---

## 5. Data Management & State

*   **Persistent State:**
    *   Its own operational configuration (loaded from YAML).
    *   (Potentially, if not using an external HA NATS JetStream cluster) NATS JetStream may persist stream data on the Orchestration Layer's disk.
*   **In-Memory State:**
    *   List of managed Simulation Worker processes and their status (Process ID, `simulation_id` they are running, connection info, health).
    *   Active client subscriptions for NATS event streams.
    *   Internal queues or buffers for message routing if not directly relying on NATS for all buffering.
    *   Current simulation configurations for active runs.
*   **Data Consistency:** Handled using Go's concurrency primitives (mutexes for shared state like worker lists, channels for message passing between goroutines).

---

## 6. Scalability & Performance Considerations

*   **Expected Load:** Handling events from potentially many (dozens to hundreds in future) Rust Simulation Workers, and fanning out to multiple UI Layers, Agent Intelligence Layer instances, Logging Layer instances. High message throughput.
*   **Potential Bottlenecks:**
    *   NATS server performance (if self-hosted and under-resourced).
    *   gRPC request handling if many synchronous control commands.
    *   CPU usage for managing many worker processes and routing messages.
    *   Network bandwidth.
*   **Strategies for Scaling/Performance:**
    *   Efficient use of goroutines for concurrent handling of connections and messages.
    *   Optimized message routing logic.
    *   Using a performant NATS cluster (for future).
    *   The Orchestration Layer itself can be designed to be stateless where possible for future horizontal scaling (multiple instances running behind a load balancer - complex setup). For Alpha, a single robust instance is the goal.
    *   Batching messages where appropriate if sending to external systems (though NATS handles individual message delivery well).
*   **Memory Usage:** Dependent on number of connected workers, active subscriptions, and in-flight messages/queue sizes.

---

## 7. Testing Strategy (Aligned with Alpha Roadmap `a0.3.Y.Z`)

*   **Unit Tests (`a0.3.1t`, etc.):** Config loading, NATS connection utilities, individual gRPC method handler logic (with mocked dependencies), worker spawning utilities.
*   **Integration Tests (Intra-layer - `a0.3.2.3`, `a0.3.5.3`):** Test gRPC services with mock clients. Test interaction between Worker Manager, gRPC handlers, and NATS publishing logic using internal mocks or local NATS.
*   **Integration Tests (Inter-layer - `a0.3.4.2`, `a0.3.6.2`, and E2E in `a0.5.4.Z`, `a0.6.3.2`):**
    *   With Mock Rust Simulation Worker: Test registration, event submission to Orchestration Layer, and command reception by worker.
    *   With Mock Client (UI Layer/Experiment Management System): Test client sending commands and Orchestration Layer attempting to forward them.
*   **Performance Tests (Load Tests - part of `a0.12.2.2`):** Measure message throughput, gRPC latencies, resource usage under simulated load.

---

## 8. Future Enhancements / Open Questions

*   **Horizontal Scalability of Orchestration Layer:** Running multiple Orchestration Layer instances for High Availability and load balancing.
*   **Advanced Worker Scheduling/Resource Management:** Integration with cluster managers like Kubernetes for sophisticated worker deployment.
*   **Security:** Authentication/authorization for client connections and worker registrations.
*   **Dynamic Reconfiguration:** Ability to update some Orchestration Layer settings or worker pool definitions without a full restart.
*   **Distributed Tracing:** Implementing tracing across gRPC calls and NATS messages.
*   **Decision on Worker Control Mechanism:** Finalize if commands to workers are via worker-specific NATS topics or via Orchestration Layer making gRPC calls to a service exposed by each Worker. (Alpha `a0.3.6.1` and `a0.5.3.1` lean towards worker-specific NATS command topics).

---

## Alpha Roadmap Version Breakdown for Orchestration Layer Development:

This section explicitly lists the tasks from the main Alpha Roadmap (`alpha.md`) that pertain to the development and integration of the Orchestration Layer.

*   **`a0.2.Y.Z`: Communication Layer - Core Message Schemas & Interface Definitions**
    *   `a0.2.2.Z`: Define Initial NATS Topic Structures & gRPC Service Signatures
        *   (`a0.2.2.1t` & `a0.2.2.1` through `a0.2.2.4t` & `a0.2.2.4` - Defines the gRPC services this Orchestration Layer will *implement* and NATS topics it will *use*).
*   **`a0.3.Y.Z`: Orchestration Layer (Go) - Basic Worker Connection & Event Ingestion/Routing**
    *   `a0.3.1.Z`: Basic Go Service Setup & Configuration
        *   (`a0.3.1.1t` & `a0.3.1.1` through `a0.3.1.4t` & `a0.3.1.4` as detailed previously)
    *   `a0.3.2.Z`: Implement gRPC Server for Worker Interaction
        *   (`a0.3.2.1t` & `a0.3.2.1` through `a0.3.2.3t` & `a0.3.2.3` as detailed previously)
    *   `a0.3.3.Z`: Simulation Worker Lifecycle Management (Minimal - for one worker)
        *   (`a0.3.3.1t` & `a0.3.3.1` through `a0.3.3.3t` & `a0.3.3.3` as detailed previously)
    *   `a0.3.4.Z`: Event Ingestion from Worker & NATS Publishing
        *   (`a0.3.4.1t` & `a0.3.4.1` through `a0.3.4.2t` & `a0.3.4.2` as detailed previously)
    *   `a0.3.5.Z`: Implement gRPC Server for Client Control Commands (Basic)
        *   (`a0.3.5.1t` & `a0.3.5.1` through `a0.3.5.3t` & `a0.3.5.3` as detailed previously)
    *   `a0.3.6.Z`: Command Forwarding to Worker (Basic)
        *   (`a0.3.6.1t` & `a0.3.6.1` through `a0.3.6.2t` & `a0.3.6.2` as detailed previously)
*   **Integration with later stages:** The Orchestration Layer will be crucial for `a0.5` (Simulation Engine Integration), `a0.6` (Experiment Management System), `a0.7` (Logging Layer), `a0.8` (Agent Intelligence Layer), `a0.9` (Evolution Engine), `a0.10` (UI Layer).

---