# 11. Monitoring and Alerting System - Design Document

**Version:** 2 (Revised for Alpha Roadmap Alignment)
**Last Updated:** 5/10/2025

---

## 1. Overview & Purpose

*   The Monitoring and Alerting System is responsible for observing the health, performance, and operational status of all components within the Fish Eco Sim platform. It collects metrics, facilitates their visualization through dashboards, and notifies relevant personnel when predefined critical conditions or anomalies occur.
*   Its primary role is to provide real-time visibility into the system's runtime behavior, help diagnose issues proactively, ensure system stability (especially during long-running simulations, Deep Reinforcement Learning training, or Genetic Algorithm evolutionary processes), and minimize downtime or wasted computational resources.
*   Key goals include: comprehensive metric collection from all layers (Simulation Layer, Orchestration Layer, Agent Intelligence Layer, etc.), effective visualization of system state, timely and actionable alerts for critical issues, and providing data to aid in performance optimization and capacity planning.

---

## 2. Core Components & Functionality (Not explicitly targeted for a dedicated Alpha `a0.X` milestone, but foundational aspects like Prometheus metric exposure will be part of individual service development, e.g., `a0.3.X.X` for Orchestration Layer)

*   **Metrics Collection Agents/Endpoints:**
    *   **Prometheus Exporters:** Core services (Go Orchestration Layer, Rust Simulation Workers (if feasible for long-running workers), Python Agent Intelligence Layer, Python Logging Layer, other Python services) will expose an HTTP `/metrics` endpoint in Prometheus exposition format.
        *   Go: `client_golang/prometheus` library.
        *   Python: `prometheus_client` library.
        *   Rust: `prometheus` crate.
    *   **NATS-based Metric/Alert Events (Alternative/Complementary for specific alerts):** Services can publish critical, discrete alert-worthy events (e.g., Protobuf messages) to dedicated NATS topics. A Custom Alerting Service (see below) would subscribe to these.
*   **Time-Series Database (TSDB) for Metrics:**
    *   **Prometheus Server:** The primary TSDB. It scrapes metrics from the exposed `/metrics` endpoints at configured intervals, stores these time-series data, and evaluates alerting rules.
    *   (Future Beta/GA - Optional) Other TSDBs like InfluxDB or TimescaleDB could be integrated if Prometheus has limitations for specific metric types or retention needs.
*   **Visualization & Dashboarding Tool:**
    *   **Grafana:** Connects to the Prometheus Server (and/or other TSDBs) as a data source.
    *   Used to create interactive dashboards for visualizing:
        *   System health metrics (CPU, memory, disk, network usage per service/worker).
        *   Application-specific metrics (e.g., Orchestration Layer queue lengths, Simulation Layer tick rates, Agent Intelligence Layer training loss/reward, Logging Layer event throughput).
        *   Error rates, gRPC/NATS latencies.
*   **Alerting Engine:**
    *   **Prometheus Alertmanager:** Integrated with Prometheus Server. Prometheus evaluates alerting rules; if an alert fires, it sends notifications to Alertmanager.
    *   **Alertmanager:** Manages alert lifecycle: deduplication, grouping, silencing, and routing alerts to various configured notification channels.
    *   **Custom Alerting Service (Python/Go - for event-driven alerts, if needed):**
        *   Subscribes to specific NATS topics where critical application-level events (not easily expressed as Prometheus metrics/thresholds) are published (e.g., `system.error.critical`, `simulation_worker.process_crash`, `drl_training.diverged_irrecoverably`).
        *   When such an event is received, this service can trigger an alert directly to a notification channel or integrate with Alertmanager.
*   **Notification Channels:**
    *   Configured in Alertmanager or the Custom Alerting Service.
    *   Examples: Email, Slack, PagerDuty.

---

## 3. Key Technologies & Patterns

*   **Primary Language(s):** Go/Python (for Custom Alerting Service, metrics exporters within existing services), Configuration languages (PromQL for Prometheus queries/alerts, YAML for tool configurations).
*   **Key Libraries/Frameworks:**
    *   **Prometheus:** Core metrics collection, storage, querying, rule-based alerting.
    *   **Grafana:** Dashboarding and visualization.
    *   **Alertmanager:** Alert routing and management.
    *   Prometheus client libraries for Go, Python, Rust (as used by respective services).
    *   NATS client libraries (if using NATS for specific alert events).
    *   Libraries for notification channels (e.g., Python `smtplib` for email, Slack SDKs).
*   **Architectural Patterns Used:**
    *   Metrics Collection (Pull-based via Prometheus scrape; Push-based for NATS alert events).
    *   Time-Series Data Storage.
    *   Threshold-based Alerting (Prometheus).
    *   Event-driven Alerting (Custom Service + NATS).
*   **Data Formats Handled:** Prometheus exposition format, Time-series data within Prometheus TSDB, JSON/YAML for alert configurations.

---

## 4. Interactions & Interfaces

*   **Interaction with All Service Layers (Simulation Layer, Orchestration Layer, Agent Intelligence Layer, Logging Layer, UI Layer if exposing relevant metrics):**
    *   These layers **expose** a `/metrics` endpoint for Prometheus to scrape.
    *   These layers may **publish** critical error or specific alert-triggering events to designated NATS topics if event-based alerting is implemented.
*   **Interaction with Prometheus Server:**
    *   Prometheus Server **scrapes** `/metrics` endpoints from all configured services.
*   **Interaction with Grafana:**
    *   Grafana **queries** Prometheus Server (or other TSDBs) to populate dashboards.
*   **Interaction with Alertmanager:**
    *   Prometheus Server **sends** fired alerts to Alertmanager.
    *   A Custom Alerting Service (if implemented) may also send alerts to Alertmanager or directly to notification channels.
*   **Interaction with Notification Channels (Email, Slack, etc.):**
    *   Alertmanager (or Custom Alerting Service) **sends** notifications to end-users/administrators.
*   **Interaction with Configuration Management System:**
    *   Reads configuration for:
        *   Prometheus Server (scrape targets, alerting rules).
        *   Grafana (dashboard definitions can be version controlled as JSON and provisioned).
        *   Alertmanager (routing rules, receiver configurations).
        *   Custom Alerting Service (NATS topics to subscribe to, specific alert conditions).

---

## 5. Data Management & State

*   **Persistent State:**
    *   Time-series metric data stored by Prometheus Server. This can become very large over time, requiring retention policies.
    *   Configurations for Prometheus, Grafana, Alertmanager.
    *   Alert history (managed by Alertmanager or through logging of alert notifications).
*   **In-Memory State:**
    *   Current alert states within Alertmanager.
    *   Recent metrics in Prometheus Server before being flushed to disk.

---

## 6. Scalability & Performance Considerations

*   **Metric Cardinality:** High cardinality (many unique label combinations for metrics) can strain Prometheus Server. Metrics should be designed thoughtfully.
*   **Scrape Interval & Volume:** Frequent scraping of many targets generating a high volume of metrics can load the Prometheus Server and the network.
*   **TSDB Storage:** Long-term storage of high-resolution metrics requires significant disk space. Prometheus has configurable retention policies.
*   **Alerting Storms:** Misconfigured alerts or cascading system failures can generate a large number of alerts. Alertmanager's grouping and inhibition features are important.
*   **Strategies for Scaling (Mostly Future Beta/GA):**
    *   Prometheus federation or remote write capabilities for very large setups.
    *   Sampling or aggregating metrics at the source if cardinality/volume is excessively high.
    *   Properly resourcing the Prometheus Server instance.

---

## 7. Testing Strategy

*   **Unit Tests (for Custom Alerting Service, if built):**
    *   Test logic for parsing alert events from NATS.
    *   Test logic for formatting and triggering notifications.
*   **Integration Tests:**
    *   Verify that individual services (Orchestration Layer, Simulation Layer, etc.) correctly expose metrics in Prometheus format on their `/metrics` endpoint.
    *   Configure a test Prometheus Server to scrape a mock service; verify metrics are collected and queryable.
    *   Define a sample alerting rule in Prometheus, trigger the condition using a mock service, and verify Alertmanager receives the alert and routes it to a mock receiver (or a test email/Slack channel).
    *   If using a Custom Alerting Service, publish mock critical events to NATS and verify notifications are triggered.
*   **Dashboard Validation:** Manually verify Grafana dashboards display expected data and visualizations correctly when connected to a test Prometheus instance with data.

---

## 8. Future Enhancements / Open Questions

*   **Distributed Tracing (e.g., Jaeger, OpenTelemetry):** For tracing requests across multiple services to diagnose latency issues and understand complex call flows.
*   **Centralized Log Aggregation & Correlation (e.g., ELK Stack - Elasticsearch, Logstash, Kibana; or Grafana Loki):** For centralizing, searching, and correlating *operational logs* (not the simulation event Parquet logs) from all services with metrics and alerts.
*   **More Sophisticated Anomaly Detection in Metrics:** Using machine learning-based tools.
*   **Automated Remediation Actions in response to specific alerts (use with extreme caution).**
*   **Defining comprehensive Service Level Objectives (SLOs)** and alerting based on them.

---

## Alpha Roadmap Version Breakdown for Monitoring & Alerting System Development:

The Monitoring & Alerting System is foundational and parts of it will be implemented alongside other services rather than in a single dedicated `a0.X` block for Alpha. Key services will start exposing metrics as they are built. Full dashboarding and complex alerting is more of a Beta concern.

*   **`a0.3.Y.Z`: Orchestration Layer (Go) - Basic Worker Connection & Event Ingestion/Routing**
    *   (Task within this): Orchestration Layer exposes basic Prometheus metrics (e.g., number of connected workers, messages processed).
*   **Individual Service Development (`a0.4` through `a0.10`):**
    *   Each new service (Rust Simulation Worker, Python Agent Intelligence Layer, Python Logging Layer, etc.) should include tasks to expose essential health and performance metrics via a Prometheus endpoint as part of its Alpha development.
*   **`a0.12.Y.Z`: Alpha PoC Finalization, Comprehensive Testing, & Documentation Review**
    *   `a0.12.2.Z`: (Task within this) Setup a basic Prometheus server to scrape metrics from all Alpha services. Create a very simple Grafana dashboard for key PoC health indicators. Define 1-2 critical alerts in Alertmanager (e.g., service down).

The full, rich dashboarding and alerting setup will be a focus for the Beta phase. Alpha aims to get the *metric exposure* in place and a minimal monitoring backbone.

---