# 7. Analysis Layer - Design Document

**Version:** 2 (Revised for Alpha Roadmap Alignment)
**Last Updated:** 5/10/2025

---

## 1. Overview & Purpose

*   The Statistical Interpretation / Analysis Layer (Analysis Layer) encompasses the tools, scripts, and processes used to perform offline statistical interpretation, visualization, and in-depth analysis of the historical data logged during Fish Eco Sim simulation runs. This layer primarily uses **Python** and its data science ecosystem.
*   Its primary role is to extract meaningful insights, patterns, and quantitative results from the raw simulation event data (stored as Apache Parquet files by the Logging Layer). This includes understanding agent behaviors, population dynamics, resource utilization, the effectiveness of Reinforcement Learning agent policies, and the impact of different simulation parameters or evolutionary changes driven by the Evolution Engine.
*   Key goals include: enabling comprehensive data exploration, supporting reproducible analyses, generating clear visualizations and reports, providing data for debugging simulation logic, and supplying quantitative results for research or fitness calculation within the Evolution Engine.

---

## 2. Core Components & Functionality (Typical Workflow & Tools - No specific Alpha `a0.X` target, but used by `a0.8` and `a0.9`)

*   **Data Ingestion & Preparation Scripts (Python):**
    *   Scripts to locate and load relevant Apache Parquet files (e.g., for specific `simulation_id`s, date ranges, or experiment sets).
    *   Utilizes Pandas (or Dask for larger datasets) for reading Parquet into DataFrames.
    *   **Payload Decoding:** Logic to deserialize the `event_payload` (binary FlatBuffer/Protocol Buffer data from the `GenericEventRequest`) into structured Python objects or Pandas columns *for specific event types* that require detailed inspection. This is performed on-demand during analysis.
    *   Data cleaning: Handling potential inconsistencies or missing values.
    *   Data transformation: Creating new derived features (e.g., calculating rates, durations), aggregating data by time windows or agent groups.
    *   Filtering: Selecting subsets of data based on `event_type_key`, agent IDs, time ranges, etc.
*   **Statistical Analysis Scripts/Notebooks (Python, potentially R):**
    *   Implementation of various statistical analyses using libraries like SciPy, Statsmodels.
        *   Descriptive statistics (mean, median, variance, distributions).
        *   Time-series analysis.
        *   Hypothesis testing.
        *   Correlation analysis.
    *   Often developed and run within Jupyter Notebooks or JupyterLab for interactive exploration.
*   **Visualization Tools & Libraries (Python, potentially R):**
    *   Generation of plots, charts, using libraries like Matplotlib, Seaborn, Plotly, Bokeh.
        *   Line charts (e.g., agent population over time).
        *   Histograms/Density plots (e.g., agent lifespans).
        *   Scatter plots, bar charts, heatmaps.
*   **Querying Engines (for future scalability with Data Lakes/Warehouses):**
    *   If Apache Parquet files are stored in cloud object storage, tools like AWS Athena, Google BigQuery, or Apache Spark (via PySpark) can be used for SQL-based querying.
*   **Reporting Tools:**
    *   Scripts to generate summary reports (e.g., HTML/PDF from Jupyter Notebooks, Markdown).
    *   (Future Beta) Custom analysis dashboards using Streamlit or Dash.
*   **Fitness Calculation Module (Python - specifically for the Evolution Engine `a0.9`):**
    *   A dedicated set of analysis scripts that processes logged Apache Parquet data for specified `simulation_id`(s (representing a genome's evaluation runs).
    *   Computes one or more fitness scores based on predefined metrics (e.g., average lifespan, total energy gathered, offspring produced).
    *   This module is invoked programmatically by the Evolution Engine.

---

## 3. Key Technologies & Patterns

*   **Primary Language(s):** Python (dominant), R (alternative/complementary).
*   **Key Libraries/Frameworks (Python):**
    *   **Pandas:** Core data manipulation and analysis.
    *   **NumPy:** Numerical computation.
    *   **PyArrow:** For reading/writing Apache Parquet files.
    *   **SciPy:** Scientific and statistical functions.
    *   **Matplotlib, Seaborn, Plotly, Bokeh:** Visualization.
    *   **Jupyter Notebook / JupyterLab:** Interactive development.
    *   **Dask:** (Future Beta/GA) For parallel processing of very large datasets.
    *   **Scikit-learn:** (If traditional machine learning models are used for analysis).
    *   `protobuf`, `flatbuffers` Python libraries (for decoding `event_payload`s).
*   **Architectural Patterns Used:**
    *   Batch Processing (analyses are run on completed log data).
    *   Data Analysis Pipelines (scripts that perform sequential data loading, transformation, analysis, visualization).
    *   Exploratory Data Analysis (EDA).
*   **Data Formats Handled (Input):** Apache Parquet files. (Internal: Pandas DataFrames, NumPy arrays).

---

## 4. Interactions & Interfaces

*   **Interaction with Logging Layer (`a0.7.Y.Z`):**
    *   Consumes the Apache Parquet files generated by the Logging Layer. This is its primary data input.
*   **Interaction with Configuration Management System (`a0.1.Y.Z`, `a0.11.Y.Z`):**
    *   Analysis scripts may read YAML configuration files that define parameters for specific analyses (e.g., paths to log data, thresholds, plotting options).
*   **Interaction with Experiment Management System (`a0.6.Y.Z`):**
    *   Analysis is often performed on data from experiments managed by the Experiment Management System. The EMS provides the link between experimental parameters and the `simulation_id`s (and thus, the log files).
    *   (Future Beta) Analysis results might be logged back to an experiment tracking platform (e.g., MLflow) managed by or integrated with the EMS.
*   **Interaction with Evolution Engine (`a0.9.Y.Z`):**
    *   The Evolution Engine invokes the Fitness Calculation Module (part of this Analysis Layer) to get fitness scores for evaluated genomes, providing the relevant `simulation_id`(s).
*   **Interaction with UI Layer (Indirectly, for Future Beta - Historical Display/Analysis in UI):**
    *   If the UI Layer incorporates advanced historical analysis or replay features, it might use simplified versions of the analysis scripts or directly query processed data prepared by this layer.

---

## 5. Data Management & State

*   **Persistent State:**
    *   The Apache Parquet log files are the primary input data.
    *   Generated analysis artifacts: reports (HTML, PDF, Markdown), plots (image files), summary statistics files (CSV, JSON), potentially trained analytical models.
    *   Analysis scripts and Jupyter notebooks themselves are critical state and should be version-controlled.
*   **In-Memory State:** Pandas DataFrames, NumPy arrays, variables during script execution. Typically ephemeral per analysis session.

---

## 6. Scalability & Performance Considerations

*   **Data Volume:** The volume of Apache Parquet data can become very large.
*   **Query/Processing Performance:** Complex analyses on large datasets can be slow.
*   **Strategies for Scaling/Performance:**
    *   Efficient Apache Parquet usage: partitioning, predicate pushdown, selective column loading.
    *   Sampling data for initial exploration.
    *   (Future Beta/GA) Distributed computing frameworks like Dask or Apache Spark for datasets exceeding single-machine memory/CPU capabilities.
    *   (Future Beta/GA) Utilizing data warehouses or query engines optimized for Parquet if SQL-based access is preferred for very large scales.
*   **Memory Usage:** Pandas DataFrames can be memory-intensive. Careful data type selection and chunked processing may be needed.

---

## 7. Testing Strategy

*   **Unit Tests (for analysis utility functions):**
    *   Test functions that decode specific `event_payload` types from binary.
    *   Test data transformation or aggregation functions with known inputs.
    *   Test statistical calculation functions against known results or reference implementations.
    *   Test fitness calculation logic with sample input data.
*   **Integration Tests (for analysis pipelines):**
    *   Test a small, complete analysis pipeline using sample Apache Parquet input files. Verify the correctness of generated outputs (e.g., summary statistics, plot data points).
*   **Reproducibility:**
    *   Ensure analysis scripts are version-controlled alongside the simulation code.
    *   Document Python package dependencies (`requirements.txt` or Conda environment files).
    *   Use fixed seeds for any stochastic processes within the analysis itself (if applicable).
*   **Validation:** Compare analysis results against known outcomes from simpler test scenarios or theoretical models where possible.

---

## 8. Future Enhancements / Open Questions

*   **Automated Reporting Pipelines:** Scripts that run regularly to generate standard reports on simulation trends.
*   **Integration with Experiment Tracking Platforms (e.g., MLflow, Weights & Biases):** To automatically log analysis results, visualizations, and link them to specific experiments and parameters.
*   **Development of an Interactive Analysis Dashboard (e.g., using Streamlit, Dash, or a Business Intelligence tool).**
*   **Advanced Anomaly Detection algorithms for simulation data.**
*   **Strategies for managing and versioning very large Apache Parquet datasets (Data Lake Management).**

---

## Alpha Roadmap Version Breakdown for Analysis Layer Usage:

While the Analysis Layer itself is a collection of offline tools and scripts rather than a service with its own `a0.X` development track in Alpha, its capabilities are crucial for other Alpha milestones:

*   **`a0.7.Y.Z`: Logging Layer (Python) - Integrated Event Capture to Parquet**
    *   (Produces the Parquet data that this Analysis Layer will consume).
*   **`a0.8.Y.Z`: Agent Intelligence Layer (Python) - Basic DRL Neural Net & Training Loop**
    *   `a0.8.5.Z`: Initial End-to-End Learning Test (Requires basic analysis of logged rewards/metrics to determine if learning is occurring).
*   **`a0.9.Y.Z`: Evolution Engine (Python) - Basic Genome & Fitness Evaluation Loop**
    *   `a0.9.2.Z`: Population Management & Fitness Evaluation Orchestration (Requires a "Fitness Calculation Module" from this Analysis Layer to process logged Parquet data and return fitness scores).
    *   `a0.9.4.Z`: Basic Generational Loop & Integration Test (Relies on fitness scores from analysis to observe evolutionary trends).
*   **`a0.12.Y.Z`: Alpha PoC Finalization, Comprehensive Testing, & Documentation Review**
    *   `a0.12.1.Z`: Comprehensive End-to-End (E2E) System Testing (Analysis of logged data will be part of verifying E2E scenarios).
    *   `a0.12.2.Z`: Performance Baselines (Analysis of performance metrics from logs).

The development of specific analysis scripts will occur as needed to support these Alpha milestones, particularly for fitness calculation and basic DRL progress verification. More sophisticated analysis tools are typically a Beta phase focus.

---