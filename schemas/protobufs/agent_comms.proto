// File: schemas/protobufs/agent_comms.proto
syntax = "proto3";

// Package declaration for generated code.
// Using a sub-package of the one in generic_event.proto if desired,
// or a new one specific to agent interactions.
package fes.simulation.agent; // fish_eco_sim.simulation.agent

// --- AgentObservation ---
// Sent from the Simulation Engine to an AI Agent, describing what the agent perceives.

// Represents a single perceived entity or stimulus in the agent's vicinity.
message Percept {
  enum EntityType {
    ENTITY_TYPE_UNSPECIFIED = 0; // Default, should not typically be used.
    ENTITY_TYPE_FOOD = 1;
    ENTITY_TYPE_AGENT = 2;       // For Alpha, could be any other agent.
                                 // Later, could be SAME_SPECIES, PREDATOR, PREY, etc.
    ENTITY_TYPE_WALL_OBSTACLE = 3; // Represents an impassable boundary or obstacle.
  }

  EntityType entity_type = 1;

  // Optional: Unique ID of the perceived entity, if available and stable.
  // Can be empty if the entity is transient or ID is not relevant for this percept.
  string entity_id = 2;

  // Position of the perceived entity relative to the observing agent's current position.
  // X-axis typically aligned with agent's forward direction or world X-axis depending on convention.
  // For Alpha, let's assume world-relative offsets from agent's position for simplicity,
  // or it could be in agent's local coordinate frame (forward is +Y, right is +X).
  // Let's define it as: X relative to agent, Y relative to agent.
  // If agent faces +Y world, then percept.relative_pos_y > 0 is in front.
  float relative_pos_x = 3;
  float relative_pos_y = 4;

  // Consider adding distance if sensor provides it directly, otherwise it can be calculated.
  // float distance = 5;
}

// Contains all information an agent observes about itself and its environment
// at a specific point in time. This is the input to the agent's decision-making process.
message AgentObservation {
  // Agent's own state
  float current_energy = 1;       // Current energy level of the agent.
  int64 current_age_ticks = 2;    // Age of the agent in simulation ticks.
  float pos_x = 3;                // Agent's absolute X position in the world.
  float pos_y = 4;                // Agent's absolute Y position in the world.
  float orientation_degrees = 5;  // Agent's current orientation in degrees (e.g., 0-360, with 0 being world +X or +Y).

  // Sensory input: A list of perceived entities/stimuli.
  repeated Percept percepts = 6;

  // Optional: Reward received by the agent for its previous action.
  // Often included in observations for reinforcement learning agents.
  // float last_reward = 7; // For Alpha v1, maybe not essential if reward is handled separately.
                         // Let's include it as it's very common for DRL.
  float last_reward = 7;
}


// --- AgentAction ---
// Sent from an AI Agent to the Simulation Engine, specifying the agent's desired action.

// Describes the action an agent intends to take in the next simulation step.
message AgentAction {
  // Desired change in forward/backward movement.
  // Positive values typically mean forward, negative could mean backward or be ignored
  // if agents can only move forward. The magnitude can be speed or a normalized thrust.
  float move_delta = 1;

  // Desired change in orientation in degrees.
  // Positive values for turning one way (e.g., counter-clockwise), negative for the other.
  float turn_delta_degrees = 2;

  // If true, the agent attempts to consume food if available at its current location/interaction range.
  bool attempt_eat = 3;

  // --- Potential future actions (not for initial Alpha AI) ---
  // bool attempt_reproduce = 4;
  // string custom_action_command = 5; // For highly specific or scripted actions
}